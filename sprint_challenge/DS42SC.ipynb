{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gutierrez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "clean_string = whitespace_string.strip()\n",
    "clean_string = (' ').join(clean_string.split())\n",
    "print(clean_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "dates = '''March 8, 2015\n",
    "March 15, 2015\n",
    "March 22, 2015\n",
    "March 29, 2015\n",
    "April 5, 2015\n",
    "April 12, 2015\n",
    "April 19, 2015\n",
    "April 26, 2015\n",
    "May 3, 2015\n",
    "May 10, 2015\n",
    "May 17, 2015\n",
    "May 24, 2015\n",
    "May 31, 2015\n",
    "June 7, 2015\n",
    "June 14, 2015\n",
    "June 21, 2015\n",
    "June 28, 2015\n",
    "July 5, 2015\n",
    "July 12, 2015\n",
    "July 19, 2015'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('March', '8', '2015'), ('March', '15', '2015'), ('March', '22', '2015'), ('March', '29', '2015'), ('April', '5', '2015'), ('April', '12', '2015'), ('April', '19', '2015'), ('April', '26', '2015'), ('May', '3', '2015'), ('May', '10', '2015'), ('May', '17', '2015'), ('May', '24', '2015'), ('May', '31', '2015'), ('June', '7', '2015'), ('June', '14', '2015'), ('June', '21', '2015'), ('June', '28', '2015'), ('July', '5', '2015'), ('July', '12', '2015'), ('July', '19', '2015')]\n"
     ]
    }
   ],
   "source": [
    "regex = r\"([a-zA-Z]+) ([\\d]+), (\\d{4})\"\n",
    "\n",
    "search_result = re.findall(regex, dates)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month Day  Year\n",
       "0   March   8  2015\n",
       "1   March  15  2015\n",
       "2   March  22  2015\n",
       "3   March  29  2015\n",
       "4   April   5  2015\n",
       "5   April  12  2015\n",
       "6   April  19  2015\n",
       "7   April  26  2015\n",
       "8     May   3  2015\n",
       "9     May  10  2015\n",
       "10    May  17  2015\n",
       "11    May  24  2015\n",
       "12    May  31  2015\n",
       "13   June   7  2015\n",
       "14   June  14  2015\n",
       "15   June  21  2015\n",
       "16   June  28  2015\n",
       "17   July   5  2015\n",
       "18   July  12  2015\n",
       "19   July  19  2015"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dates = pd.DataFrame(search_result, columns=['Month', 'Day', 'Year'])\n",
    "df_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['                     is so sad for my APL friend.............',\n",
       "       '                   I missed the New Moon trailer...',\n",
       "       '              omg its already 7:30 :O',\n",
       "       \"          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\",\n",
       "       '         i think mi bf is cheating on me!!!       T_T'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.SentimentText.values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    # make all text lowercase\n",
    "    doc = str(doc).lower()\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split(' ')\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "df['SentimentText'] = df['SentimentText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [sad, apl, friend]\n",
       "1                         [missed, new, moon, trailer]\n",
       "2                                       [omg, already]\n",
       "3    [omgaga, im, sooo, im, gunna, cry, ive, dentis...\n",
       "4                        [think, mi, bf, cheating, tt]\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.SentimentText[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "#### Your Answer Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF creates a matrix of words in documents provided where words are assigned a score between 0 and 1.\n",
    "Words with higher relevance are given a higer score, while those with less relevance have a lower score.\n",
    "Scores are calculated by taking the frequency of a word in a document divided by the number of documents that word is in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining text to make sentence\n",
    "def join_text(doc):\n",
    "    joined_doc = (' ').join(doc)\n",
    "    return joined_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText1'] = df['SentimentText'].apply(join_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>SentimentText1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already]</td>\n",
       "      <td>omg already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "      <td>omgaga im sooo im gunna cry ive dentist since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "      <td>think mi bf cheating tt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                                 [sad, apl, friend]   \n",
       "1          0                       [missed, new, moon, trailer]   \n",
       "2          1                                     [omg, already]   \n",
       "3          0  [omgaga, im, sooo, im, gunna, cry, ive, dentis...   \n",
       "4          0                      [think, mi, bf, cheating, tt]   \n",
       "\n",
       "                                      SentimentText1  \n",
       "0                                     sad apl friend  \n",
       "1                            missed new moon trailer  \n",
       "2                                        omg already  \n",
       "3  omgaga im sooo im gunna cry ive dentist since ...  \n",
       "4                            think mi bf cheating tt  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_Test_Split to create train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.SentimentText1\n",
    "y = df.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79991,), (19998,), (79991,), (19998,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1:** Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sleep': 65, 'haha': 24, 'love': 46, 'little': 42, 'dont': 11, 'know': 38, 'wanna': 86, 'hear': 28, 'like': 41, 'got': 21, 'ill': 35, 'week': 90, 'time': 77, 'long': 44, 'thing': 74, 'good': 20, 'night': 55, 'youre': 99, 'way': 89, 'thats': 73, 'today': 78, 'yes': 98, 'cool': 6, 'right': 61, 'lol': 43, 'new': 53, 'nice': 54, 'happy': 25, 'day': 7, 'im': 36, 'sad': 62, 'come': 5, 'yeah': 97, 'doesnt': 10, 'think': 75, 'really': 60, 'want': 87, 'bad': 2, 'miss': 50, 'wont': 94, 'help': 29, 'amp': 0, 'twitter': 83, 'gonna': 19, 'let': 39, 'oh': 56, 'didnt': 9, 'ok': 57, 'thanks': 72, 'said': 63, 'work': 95, 'tomorrow': 79, 'hope': 33, 'feel': 12, 'better': 4, 'soon': 66, 'ive': 37, 'hi': 31, 'make': 47, 'going': 18, 'follow': 13, 'getting': 15, 'need': 52, 'great': 22, 'ya': 96, 'morning': 51, 'people': 58, 'id': 34, 'wait': 85, 'thank': 71, 'welcome': 92, 'ur': 84, 'life': 40, 'fun': 14, 'sorry': 67, 'look': 45, 'guys': 23, 'girl': 16, 'wish': 93, 'tonight': 80, 'tell': 70, 'home': 32, 'sounds': 68, 'thought': 76, 'awesome': 1, 'say': 64, 'hey': 30, 'sure': 69, 'havent': 27, 'watch': 88, 'best': 3, 'weekend': 91, 'hate': 26, 'days': 8, 'maybe': 49, 'glad': 17, 'try': 81, 'tweet': 82, 'pretty': 59, 'man': 48}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=100, ngram_range=(1,1), stop_words='english')\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# Vocabulary created with Count Vectorizer\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bad</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>come</th>\n",
       "      <th>cool</th>\n",
       "      <th>day</th>\n",
       "      <th>days</th>\n",
       "      <th>didnt</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>welcome</th>\n",
       "      <th>wish</th>\n",
       "      <th>wont</th>\n",
       "      <th>work</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amp  awesome  bad  best  better  come  cool  day  days  didnt  ...  week  \\\n",
       "0    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "1    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "2    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "3    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "4    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "\n",
       "   weekend  welcome  wish  wont  work  ya  yeah  yes  youre  \n",
       "0        0        0     0     0     0   0     0    0      0  \n",
       "1        0        0     0     0     0   0     0    0      0  \n",
       "2        0        0     0     0     0   0     0    0      0  \n",
       "3        0        0     0     0     0   0     0    0      0  \n",
       "4        0        0     0     0     0   0     0    0      0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts  = vectorizer.transform(X_train)\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(),\n",
    "                                 columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19998, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bad</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>come</th>\n",
       "      <th>cool</th>\n",
       "      <th>day</th>\n",
       "      <th>days</th>\n",
       "      <th>didnt</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>welcome</th>\n",
       "      <th>wish</th>\n",
       "      <th>wont</th>\n",
       "      <th>work</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amp  awesome  bad  best  better  come  cool  day  days  didnt  ...  week  \\\n",
       "0    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "1    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "2    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "3    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "4    0        0    0     0       0     0     0    0     0      0  ...     0   \n",
       "\n",
       "   weekend  welcome  wish  wont  work  ya  yeah  yes  youre  \n",
       "0        0        0     0     0     0   0     0    0      0  \n",
       "1        0        0     0     0     0   1     0    0      0  \n",
       "2        0        0     0     0     0   0     0    0      0  \n",
       "3        0        0     0     0     0   0     0    0      0  \n",
       "4        0        0     0     0     0   0     0    0      0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(),\n",
    "                                columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC_AUC Score: 0.7348323308546927\n",
      "Test ROC_AUC Score: 0.6249485470432838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier(n_estimators=200).fit(X_train_vectorized, y_train)\n",
    "\n",
    "rfc_train_predictions = RFC.predict(X_train_vectorized)\n",
    "rfc_test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "# Using ROC_AUC Score (not Accuracy)\n",
    "print(f'Train ROC_AUC Score: {roc_auc_score(y_train, rfc_train_predictions)}')\n",
    "print(f'Test ROC_AUC Score: {roc_auc_score(y_test, rfc_test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC_AUC Score: 0.6319499651220892\n",
      "Test ROC_AUC Score: 0.6265282158097948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                           n_estimators=300, max_depth=3).fit(X_train_vectorized, y_train)\n",
    "\n",
    "gbc_train_predictions = GBC.predict(X_train_vectorized)\n",
    "gbc_test_predictions = GBC.predict(X_test_vectorized)\n",
    "\n",
    "# Using ROC_AUC Score (not Accuracy)\n",
    "print(f'Train ROC_AUC Score: {roc_auc_score(y_train, gbc_train_predictions)}')\n",
    "print(f'Test ROC_AUC Score: {roc_auc_score(y_test, gbc_test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 2:** TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sleep': 65, 'haha': 24, 'love': 46, 'little': 42, 'dont': 11, 'know': 38, 'wanna': 86, 'hear': 28, 'like': 41, 'got': 21, 'ill': 35, 'week': 90, 'time': 77, 'long': 44, 'thing': 74, 'good': 20, 'night': 55, 'youre': 99, 'way': 89, 'thats': 73, 'today': 78, 'yes': 98, 'cool': 6, 'right': 61, 'lol': 43, 'new': 53, 'nice': 54, 'happy': 25, 'day': 7, 'im': 36, 'sad': 62, 'come': 5, 'yeah': 97, 'doesnt': 10, 'think': 75, 'really': 60, 'want': 87, 'bad': 2, 'miss': 50, 'wont': 94, 'help': 29, 'amp': 0, 'twitter': 83, 'gonna': 19, 'let': 39, 'oh': 56, 'didnt': 9, 'ok': 57, 'thanks': 72, 'said': 63, 'work': 95, 'tomorrow': 79, 'hope': 33, 'feel': 12, 'better': 4, 'soon': 66, 'ive': 37, 'hi': 31, 'make': 47, 'going': 18, 'follow': 13, 'getting': 15, 'need': 52, 'great': 22, 'ya': 96, 'morning': 51, 'people': 58, 'id': 34, 'wait': 85, 'thank': 71, 'welcome': 92, 'ur': 84, 'life': 40, 'fun': 14, 'sorry': 67, 'look': 45, 'guys': 23, 'girl': 16, 'wish': 93, 'tonight': 80, 'tell': 70, 'home': 32, 'sounds': 68, 'thought': 76, 'awesome': 1, 'say': 64, 'hey': 30, 'sure': 69, 'havent': 27, 'watch': 88, 'best': 3, 'weekend': 91, 'hate': 26, 'days': 8, 'maybe': 49, 'glad': 17, 'try': 81, 'tweet': 82, 'pretty': 59, 'man': 48}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=100, ngram_range=(1,1), stop_words='english')\n",
    "vectorizer_tfidf.fit(X_train)\n",
    "\n",
    "# Vocabulary created with TF-IDF Vectorizer\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bad</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>come</th>\n",
       "      <th>cool</th>\n",
       "      <th>day</th>\n",
       "      <th>days</th>\n",
       "      <th>didnt</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>welcome</th>\n",
       "      <th>wish</th>\n",
       "      <th>wont</th>\n",
       "      <th>work</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amp  awesome  bad  best  better  come  cool  day  days  didnt  ...  week  \\\n",
       "0  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "1  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "2  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "3  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "4  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "\n",
       "   weekend  welcome  wish  wont  work   ya  yeah  yes  youre  \n",
       "0      0.0      0.0   0.0   0.0   0.0  0.0   0.0  0.0    0.0  \n",
       "1      0.0      0.0   0.0   0.0   0.0  0.0   0.0  0.0    0.0  \n",
       "2      0.0      0.0   0.0   0.0   0.0  0.0   0.0  0.0    0.0  \n",
       "3      0.0      0.0   0.0   0.0   0.0  0.0   0.0  0.0    0.0  \n",
       "4      0.0      0.0   0.0   0.0   0.0  0.0   0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts_tfidf = vectorizer_tfidf.transform(X_train)\n",
    "X_train_vectorized_tfidf = pd.DataFrame(train_word_counts_tfidf.toarray(),\n",
    "                                       columns=vectorizer_tfidf.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized_tfidf.shape)\n",
    "X_train_vectorized_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19998, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bad</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>come</th>\n",
       "      <th>cool</th>\n",
       "      <th>day</th>\n",
       "      <th>days</th>\n",
       "      <th>didnt</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>welcome</th>\n",
       "      <th>wish</th>\n",
       "      <th>wont</th>\n",
       "      <th>work</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amp  awesome  bad  best  better  come  cool  day  days  didnt  ...  week  \\\n",
       "0  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "1  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "2  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "3  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "4  0.0      0.0  0.0   0.0     0.0   0.0   0.0  0.0   0.0    0.0  ...   0.0   \n",
       "\n",
       "   weekend  welcome  wish  wont  work       ya  yeah  yes  youre  \n",
       "0      0.0      0.0   0.0   0.0   0.0  0.00000   0.0  0.0    0.0  \n",
       "1      0.0      0.0   0.0   0.0   0.0  0.66453   0.0  0.0    0.0  \n",
       "2      0.0      0.0   0.0   0.0   0.0  0.00000   0.0  0.0    0.0  \n",
       "3      0.0      0.0   0.0   0.0   0.0  0.00000   0.0  0.0    0.0  \n",
       "4      0.0      0.0   0.0   0.0   0.0  0.00000   0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "X_test_vectorized_tfidf = pd.DataFrame(test_word_counts_tfidf.toarray(),\n",
    "                                      columns=vectorizer_tfidf.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized_tfidf.shape)\n",
    "X_test_vectorized_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC_AUC Score: 0.7353071990655697\n",
      "Test ROC_AUC Score: 0.6291072139808982\n"
     ]
    }
   ],
   "source": [
    "RFC_tfidf = RandomForestClassifier(n_estimators=100).fit(X_train_vectorized_tfidf, y_train)\n",
    "train_predictions_tfidf = RFC_tfidf.predict(X_train_vectorized_tfidf)\n",
    "test_predictions_tfidf = RFC_tfidf.predict(X_test_vectorized_tfidf)\n",
    "\n",
    "# Using ROC_AUC Score (not Accuracy)\n",
    "print(f'Train ROC_AUC Score: {roc_auc_score(y_train, train_predictions_tfidf)}')\n",
    "print(f'Test ROC_AUC Score: {roc_auc_score(y_test, test_predictions_tfidf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC_AUC Score: 0.6318801944829562\n",
      "Test ROC_AUC Score: 0.6269408351960983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC_tfidf = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                           n_estimators=300, max_depth=3).fit(X_train_vectorized, y_train)\n",
    "\n",
    "gbc_train_predictions_tfidf = GBC_tfidf.predict(X_train_vectorized)\n",
    "gbc_test_predictions_tfidf = GBC_tfidf.predict(X_test_vectorized)\n",
    "\n",
    "# Using ROC_AUC Score (not Accuracy)\n",
    "print(f'Train ROC_AUC Score: {roc_auc_score(y_train, gbc_train_predictions_tfidf)}')\n",
    "print(f'Test ROC_AUC Score: {roc_auc_score(y_test, gbc_test_predictions_tfidf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(df.SentimentText, min_count=20, window=3, size=300, negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('account', 0.7636793851852417),\n",
       " ('facebook', 0.7506049871444702),\n",
       " ('page', 0.7476189732551575),\n",
       " ('via', 0.7369566559791565),\n",
       " ('list', 0.7338579893112183),\n",
       " ('web', 0.7260967493057251),\n",
       " ('updates', 0.7207168936729431),\n",
       " ('myspace', 0.7120668292045593),\n",
       " ('app', 0.7016830444335938),\n",
       " ('others', 0.6991567611694336)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"twitter\"], topn=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
